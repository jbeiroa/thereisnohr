{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d82a9411-0073-4b3a-a44d-b328b09ad407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from mlx_lm import load, generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c93a90de-887a-40e2-9ccc-8126198c1e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e96a33501ee64311a86ce483fa0d5d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b230c1e7-d4f9-4fd0-aa0f-59d91ba1a67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649e6ce796a04a969dc9102e7ff8fd31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ecf7bc416b4a1f86f835dbdc10c68b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = load('jbeiroa/mlx_lora_llama-3.2-1b-q8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68500120-3c95-4ffa-8045-8b94ea230525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Model(\n",
       "   (model): LlamaModel(\n",
       "     (embed_tokens): Embedding(128256, 2048)\n",
       "     (layers.0): TransformerBlock(\n",
       "       (self_attn): Attention(\n",
       "         (q_proj): Linear(input_dims=2048, output_dims=2048, bias=False)\n",
       "         (k_proj): Linear(input_dims=2048, output_dims=512, bias=False)\n",
       "         (v_proj): Linear(input_dims=2048, output_dims=512, bias=False)\n",
       "         (o_proj): Linear(input_dims=2048, output_dims=2048, bias=False)\n",
       "         (rope): Llama3RoPE()\n",
       "       )\n",
       "       (mlp): MLP(\n",
       "         (gate_proj): Linear(input_dims=2048, output_dims=8192, bias=False)\n",
       "         (down_proj): Linear(input_dims=8192, output_dims=2048, bias=False)\n",
       "         (up_proj): Linear(input_dims=2048, output_dims=8192, bias=False)\n",
       "       )\n",
       "       (input_layernorm): RMSNorm(2048, eps=1e-05)\n",
       "       (post_attention_layernorm): RMSNorm(2048, eps=1e-05)\n",
       "     )\n",
       "     (layers.1): TransformerBlock(\n",
       "       (self_attn): Attention(\n",
       "         (q_proj): Linear(input_dims=2048, output_dims=2048, bias=False)\n",
       "         (k_proj): Linear(input_dims=2048, output_dims=512, bias=False)\n",
       "         (v_proj): Linear(input_dims=2048, output_dims=512, bias=False)\n",
       "         (o_proj): Linear(input_dims=2048, output_dims=2048, bias=False)\n",
       "         (rope): Llama3RoPE()\n",
       "       )\n",
       "       (mlp): MLP(\n",
       "         (gate_proj): Linear(input_dims=2048, output_dims=8192, bias=False)\n",
       "         (down_proj): Linear(input_dims=8192, output_dims=2048, bias=False)\n",
       "         (up_proj): Linear(input_dims=2048, output_dims=8192, bias=False)\n",
       "       )\n",
       "       (input_layernorm): RMSNorm(2048, eps=1e-05)\n",
       "       (post_attention_layernorm): RMSNorm(2048, eps=1e-05)\n",
       "     )\n",
       "     (layers.2): TransformerBlock(\n",
       "       (self_attn): Attention(\n",
       "         (q_proj): Linear(input_dims=2048, output_dims=2048, bias=False)\n",
       "         (k_proj): Linear(input_dims=2048, output_dims=512, bias=False)\n",
       "         (v_proj): Linear(input_dims=2048, output_dims=512, bias=False)\n",
       "         (o_proj): Linear(input_dims=2048, output_dims=2048, bias=False)\n",
       "         (rope): Llama3RoPE()\n",
       "       )\n",
       "       (mlp): MLP(\n",
       "         (gate_proj): Linear(input_dims=2048, output_dims=8192, bias=False)\n",
       "         (down_proj): Linear(input_dims=8192, output_dims=2048, bias=False)\n",
       "         (up_proj): Linear(input_dims=2048, output_dims=8192, bias=False)\n",
       "       )\n",
       "       (input_layernorm): RMSNorm(2048, eps=1e-05)\n",
       "       (post_attention_layernorm): RMSNorm(2048, eps=1e-05)\n",
       "     )\n",
       "     (layers.3): TransformerBlock(\n",
       "       (self_attn): Attention(\n",
       "         (q_proj): Linear(input_dims=2048, output_dims=2048, bias=False)\n",
       "         (k_proj): Linear(input_dims=2048, output_dims=512, bias=False)\n",
       "         (v_proj): Linear(input_dims=2048, output_dims=512, bias=False)\n",
       "         (o_proj): Linear(input_dims=2048, output_dims=2048, bias=False)\n",
       "         (rope): Llama3RoPE()\n",
       "       )\n",
       "       (mlp): MLP(\n",
       "         (gate_proj): Linear(input_dims=2048, output_dims=8192, bias=False)\n",
       "         (down_proj): Linear(input_dims=8192, output_dims=2048, bias=False)\n",
       "         (up_proj): Linear(input_dims=2048, output_dims=8192, bias=False)\n",
       "       )\n",
       "       (input_layernorm): RMSNorm(2048, eps=1e-05)\n",
       "       (post_attention_layernorm): RMSNorm(2048, eps=1e-05)\n",
       "     )\n",
       "     (layers.4): TransformerBlock(\n",
       "       (self_attn): Attention(\n",
       "         (q_proj): Linear(input_dims=2048, output_dims=2048, bias=False)\n",
       "         (k_proj): Linear(input_dims=2048, output_dims=512, bias=False)\n",
       "         (v_proj): Linear(input_dims=2048, output_dims=512, bias=False)\n",
       "         (o_proj): Linear(input_dims=2048, output_dims=2048, bias=False)\n",
       "         (rope): Llama3RoPE()\n",
       "       )\n",
       "       (mlp): MLP(\n",
       "         (gate_proj): Linear(input_dims=2048, output_dims=8192, bias=False)\n",
       "         (down_proj): Linear(input_dims=8192, output_dims=2048, bias=False)\n",
       "         (up_proj): Linear(input_dims=2048, output_dims=8192, bias=False)\n",
       "       )\n",
       "       (input_layernorm): RMSNorm(2048, eps=1e-05)\n",
       "       (post_attention_layernorm): RMSNorm(2048, eps=1e-05)\n",
       "     )\n",
       "     (layers.5): TransformerBlock(\n",
       "       (self_attn): Attention(\n",
       "         (q_proj): Linear(input_dims=2048, output_dims=2048, bias=False)\n",
       "         (k_proj): Linear(input_dims=2048, output_dims=512, bias=False)\n",
       "         (v_proj): Linear(input_dims=2048, output_dims=512, bias=False)\n",
       "         (o_proj): Linear(input_dims=2048, output_dims=2048, bias=False)\n",
       "         (rope): Llama3RoPE()\n",
       "       )\n",
       "       (mlp): MLP(\n",
       "         (gate_proj): Linear(input_dims=2048, output_dims=8192, bias=False)\n",
       "         (down_proj): Linear(input_dims=8192, output_dims=2048, bias=False)\n",
       "         (up_proj): Linear(input_dims=2048, output_dims=8192, bias=False)\n",
       "       )\n",
       "       (input_layernorm): RMSNorm(2048, eps=1e-05)\n",
       "       (post_attention_layernorm): RMSNorm(2048, eps=1e-05)\n",
       "     )\n",
       "     (layers.6): TransformerBlock(\n",
       "       (self_attn): Attention(\n",
       "         (q_proj): Linear(input_dims=2048, output_dims=2048, bias=False)\n",
       "         (k_proj): Linear(input_dims=2048, output_dims=512, bias=False)\n",
       "         (v_proj): Linear(input_dims=2048, output_dims=512, bias=False)\n",
       "         (o_proj): Linear(input_dims=2048, output_dims=2048, bias=False)\n",
       "         (rope): Llama3RoPE()\n",
       "       )\n",
       "       (mlp): MLP(\n",
       "         (gate_proj): Linear(input_dims=2048, output_dims=8192, bias=False)\n",
       "         (down_proj): Linear(input_dims=8192, output_dims=2048, bias=False)\n",
       "         (up_proj): Linear(input_dims=2048, output_dims=8192, bias=False)\n",
       "       )\n",
       "       (input_layernorm): RMSNorm(2048, eps=1e-05)\n",
       "       (post_attention_layernorm): RMSNorm(2048, eps=1e-05)\n",
       "     )\n",
       "     (layers.7): TransformerBlock(\n",
       "       (self_attn): Attention(\n",
       "         (q_proj): Linear(input_dims=2048, output_dims=2048, bias=False)\n",
       "         (k_proj): Linear(input_dims=2048, output_dims=512, bias=False)\n",
       "         (v_proj): Linear(input_dims=2048, output_dims=512, bias=False)\n",
       "         (o_proj): Linear(input_dims=2048, output_dims=2048, bias=False)\n",
       "         (rope): Llama3RoPE()\n",
       "       )\n",
       "       (mlp): MLP(\n",
       "         (gate_proj): Linear(input_dims=2048, output_dims=8192, bias=False)\n",
       "         (down_proj): Linear(input_dims=8192, output_dims=2048, bias=False)\n",
       "         (up_proj): Linear(input_dims=2048, output_dims=8192, bias=False)\n",
       "       )\n",
       "       (input_layernorm): RMSNorm(2048, eps=1e-05)\n",
       "       (post_attention_layernorm): RMSNorm(2048, eps=1e-05)\n",
       "     )\n",
       "     (layers.8): TransformerBlock(\n",
       "       (self_attn): Attention(\n",
       "         (q_proj): Linear(input_dims=2048, output_dims=2048, bias=False)\n",
       "         (k_proj): Linear(input_dims=2048, output_dims=512, bias=False)\n",
       "         (v_proj): Linear(input_dims=2048, output_dims=512, bias=False)\n",
       "         (o_proj): Linear(input_dims=2048, output_dims=2048, bias=False)\n",
       "         (rope): Llama3RoPE()\n",
       "       )\n",
       "       (mlp): MLP(\n",
       "         (gate_proj): Linear(input_dims=2048, output_dims=8192, bias=False)\n",
       "         (down_proj): Linear(input_dims=8192, output_dims=2048, bias=False)\n",
       "         (up_proj): Linear(input_dims=2048, output_dims=8192, bias=False)\n",
       "       )\n",
       "       (input_layernorm): RMSNorm(2048, eps=1e-05)\n",
       "       (post_attention_layernorm): RMSNorm(2048, eps=1e-05)\n",
       "     )\n",
       "     (layers.9): TransformerBlock(\n",
       "       (self_attn): Attention(\n",
       "         (q_proj): Linear(input_dims=2048, output_dims=2048, bias=False)\n",
       "         (k_proj): Linear(input_dims=2048, output_dims=512, bias=False)\n",
       "         (v_proj): Linear(input_dims=2048, output_dims=512, bias=False)\n",
       "         (o_proj): Linear(input_dims=2048, output_dims=2048, bias=False)\n",
       "         (rope): Llama3RoPE()\n",
       "       )\n",
       "       (mlp): MLP(\n",
       "         (gate_proj): Linear(input_dims=2048, output_dims=8192, bias=False)\n",
       "         (down_proj): Linear(input_dims=8192, output_dims=2048, bias=False)\n",
       "         (up_proj): Linear(input_dims=2048, output_dims=8192, bias=False)\n",
       "       )\n",
       "       (input_layernorm): RMSNorm(2048, eps=1e-05)\n",
       "       (post_attention_layernorm): RMSNorm(2048, eps=1e-05)\n",
       "     )\n",
       "     (layers.10): TransformerBlock(\n",
       "       (self_attn): Attention(\n",
       "         (q_proj): Linear(input_dims=2048, output_dims=2048, bias=False)\n",
       "         (k_proj): Linear(input_dims=2048, output_dims=512, bias=False)\n",
       "         (v_proj): Linear(input_dims=2048, output_dims=512, bias=False)\n",
       "         (o_proj): Linear(input_dims=2048, output_dims=2048, bias=False)\n",
       "         (rope): Llama3RoPE()\n",
       "       )\n",
       "       (mlp): MLP(\n",
       "         (gate_proj): Linear(input_dims=2048, output_dims=8192, bias=False)\n",
       "         (down_proj): Linear(input_dims=8192, output_dims=2048, bias=False)\n",
       "         (up_proj): Linear(input_dims=2048, output_dims=8192, bias=False)\n",
       "       )\n",
       "       (input_layernorm): RMSNorm(2048, eps=1e-05)\n",
       "       (post_attention_layernorm): RMSNorm(2048, eps=1e-05)\n",
       "     )\n",
       "     (layers.11): TransformerBlock(\n",
       "       (self_attn): Attention(\n",
       "         (q_proj): Linear(input_dims=2048, output_dims=2048, bias=False)\n",
       "         (k_proj): Linear(input_dims=2048, output_dims=512, bias=False)\n",
       "         (v_proj): Linear(input_dims=2048, output_dims=512, bias=False)\n",
       "         (o_proj): Linear(input_dims=2048, output_dims=2048, bias=False)\n",
       "         (rope): Llama3RoPE()\n",
       "       )\n",
       "       (mlp): MLP(\n",
       "         (gate_proj): Linear(input_dims=2048, output_dims=8192, bias=False)\n",
       "         (down_proj): Linear(input_dims=8192, output_dims=2048, bias=False)\n",
       "         (up_proj): Linear(input_dims=2048, output_dims=8192, bias=False)\n",
       "       )\n",
       "       (input_layernorm): RMSNorm(2048, eps=1e-05)\n",
       "       (post_attention_layernorm): RMSNorm(2048, eps=1e-05)\n",
       "     )\n",
       "     (layers.12): TransformerBlock(\n",
       "       (self_attn): Attention(\n",
       "         (q_proj): Linear(input_dims=2048, output_dims=2048, bias=False)\n",
       "         (k_proj): Linear(input_dims=2048, output_dims=512, bias=False)\n",
       "         (v_proj): Linear(input_dims=2048, output_dims=512, bias=False)\n",
       "         (o_proj): Linear(input_dims=2048, output_dims=2048, bias=False)\n",
       "         (rope): Llama3RoPE()\n",
       "       )\n",
       "       (mlp): MLP(\n",
       "         (gate_proj): Linear(input_dims=2048, output_dims=8192, bias=False)\n",
       "         (down_proj): Linear(input_dims=8192, output_dims=2048, bias=False)\n",
       "         (up_proj): Linear(input_dims=2048, output_dims=8192, bias=False)\n",
       "       )\n",
       "       (input_layernorm): RMSNorm(2048, eps=1e-05)\n",
       "       (post_attention_layernorm): RMSNorm(2048, eps=1e-05)\n",
       "     )\n",
       "     (layers.13): TransformerBlock(\n",
       "       (self_attn): Attention(\n",
       "         (q_proj): Linear(input_dims=2048, output_dims=2048, bias=False)\n",
       "         (k_proj): Linear(input_dims=2048, output_dims=512, bias=False)\n",
       "         (v_proj): Linear(input_dims=2048, output_dims=512, bias=False)\n",
       "         (o_proj): Linear(input_dims=2048, output_dims=2048, bias=False)\n",
       "         (rope): Llama3RoPE()\n",
       "       )\n",
       "       (mlp): MLP(\n",
       "         (gate_proj): Linear(input_dims=2048, output_dims=8192, bias=False)\n",
       "         (down_proj): Linear(input_dims=8192, output_dims=2048, bias=False)\n",
       "         (up_proj): Linear(input_dims=2048, output_dims=8192, bias=False)\n",
       "       )\n",
       "       (input_layernorm): RMSNorm(2048, eps=1e-05)\n",
       "       (post_attention_layernorm): RMSNorm(2048, eps=1e-05)\n",
       "     )\n",
       "     (layers.14): TransformerBlock(\n",
       "       (self_attn): Attention(\n",
       "         (q_proj): Linear(input_dims=2048, output_dims=2048, bias=False)\n",
       "         (k_proj): Linear(input_dims=2048, output_dims=512, bias=False)\n",
       "         (v_proj): Linear(input_dims=2048, output_dims=512, bias=False)\n",
       "         (o_proj): Linear(input_dims=2048, output_dims=2048, bias=False)\n",
       "         (rope): Llama3RoPE()\n",
       "       )\n",
       "       (mlp): MLP(\n",
       "         (gate_proj): Linear(input_dims=2048, output_dims=8192, bias=False)\n",
       "         (down_proj): Linear(input_dims=8192, output_dims=2048, bias=False)\n",
       "         (up_proj): Linear(input_dims=2048, output_dims=8192, bias=False)\n",
       "       )\n",
       "       (input_layernorm): RMSNorm(2048, eps=1e-05)\n",
       "       (post_attention_layernorm): RMSNorm(2048, eps=1e-05)\n",
       "     )\n",
       "     (layers.15): TransformerBlock(\n",
       "       (self_attn): Attention(\n",
       "         (q_proj): Linear(input_dims=2048, output_dims=2048, bias=False)\n",
       "         (k_proj): Linear(input_dims=2048, output_dims=512, bias=False)\n",
       "         (v_proj): Linear(input_dims=2048, output_dims=512, bias=False)\n",
       "         (o_proj): Linear(input_dims=2048, output_dims=2048, bias=False)\n",
       "         (rope): Llama3RoPE()\n",
       "       )\n",
       "       (mlp): MLP(\n",
       "         (gate_proj): Linear(input_dims=2048, output_dims=8192, bias=False)\n",
       "         (down_proj): Linear(input_dims=8192, output_dims=2048, bias=False)\n",
       "         (up_proj): Linear(input_dims=2048, output_dims=8192, bias=False)\n",
       "       )\n",
       "       (input_layernorm): RMSNorm(2048, eps=1e-05)\n",
       "       (post_attention_layernorm): RMSNorm(2048, eps=1e-05)\n",
       "     )\n",
       "     (norm): RMSNorm(2048, eps=1e-05)\n",
       "   )\n",
       " ),\n",
       " <mlx_lm.tokenizer_utils.TokenizerWrapper at 0x148787b00>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ea05dba-97c3-4e85-b1c1-62e8874203a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN&l=IN'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = f\"\"\"Summarize this resume:\n",
    "Nitin Verma\\nAssisting Microsoft Partners - Exchange Online and Office\\n\\nPune, Maharashtra - Email me on Indeed: indeed.com/r/Nitin-Verma/b9e8520147f728d2\\n\\nWORK EXPERIENCE\\n\\nAssisting Microsoft Partners\\n\\nExchange Online and Office -\\n\\nSeptember 2017 to Present\\n\\nfrom around the world with AD, Exchange Online and Office 365\\nrelated issues.\\n\\u27a2 Prompt assistance within the required SLA as per the case creation by users and follow up\\ntimely till the issue has been resolved up to the user's satisfaction.\\n\\n\\u27a2 Troubleshooting and resolving various production impacting critical issues.\\n\\n\\u27a2 Educating Admins about various Microsoft Office 365 features and assisting them with\\nimplementing the same using GUI as well as PowerShell.\\n\\n\\u27a2 Reproducing the users' issues on test environment and researching to find a resolution.\\n\\n\\u27a2 Interact with other 2nd level support team for joined troubleshooting sessions where root cause\\nis\\nnot well defined.\\n\\n\\u27a2 Assisting users/admins with various Office 365 applications, like Outlook, Word, SharePoint,\\nOneDrive, Skype For Business, etc.\\n\\n\\u27a2 Brainstorming with the Technical Advisors from Microsoft regarding Service Incidents impacting\\nmultiple tenants.\\n\\nTools Used: RAVE, CAP and AVAYA.\\n\\nEDUCATION\\n\\nBachelor of Engineering in EnTC\\n\\nPune University -  Pune, Maharashtra\\n\\nhttps://www.indeed.com/r/Nitin-Verma/b9e8520147f728d2?isid=rex-download&ikw=download-top&co=IN\"\"\"\n",
    "\n",
    "generate(model=model[0], tokenizer=model[1], prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "972a86b1-0b23-434f-93c7-e3bef46911c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Einstein was a German-born theoretical physicist who developed the theory of relativity, one of the two pillars of modern physics (along with quantum mechanics). He received the 1921 Nobel Prize for Physics \"for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect\". He is best known to the general public for his massâ€“energy equivalence formula E = mc2 (which has been dubbed \"the world\\'s most famous equation\"), first proposed in 1905. He also developed the theory of Brownian motion, the theory of special relativity, and won the 1921 Nobel Prize in Physics for his work in quantum theory. Einstein is widely known for his conceptual breakthroughs, particularly for the theory of special relativity. He is also known for his opposition to war and for his efforts for peace and for his leadership of the Red Cross. Einstein is widely regarded as one of the greatest physicists of all time. He was awarded the 1921 Nobel Prize in Physics \"for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect\". He is best known to the general public for his massâ€“energy equivalence formula E = mc2 (which has been dubbed \"the world\\'s most famous equation\"),'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model=model[0], tokenizer=model[1], prompt=\"Who was Albert Einstein?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab6a9ea-c64f-4ced-9c94-52aedf47b1d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
