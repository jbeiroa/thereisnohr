{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfdddc47-ebbb-4852-8ea7-7953d35d0056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "import pymupdf\n",
    "import pymupdf4llm\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6f4f86b-fb08-47ca-9241-f9db63a3968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2040634e-cb51-4d11-b0a4-27e492baab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to demo files\n",
    "repo_path = os.path.dirname(os.getcwd())\n",
    "cvs_path = os.path.join(repo_path, 'cvs')\n",
    "cvs = [os.path.join(cvs_path, file) for file in os.listdir(cvs_path) if file.endswith('.pdf')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bba20b9b-f02a-4909-9f74-afad91f23546",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Getter:\n",
    "    \"\"\"\n",
    "    A class for processing PDF files in a specified directory and converting them to markdown format.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    directory : str\n",
    "        Path to the directory containing PDF files.\n",
    "    save_to_file : bool, optional\n",
    "        If True, saves the converted markdown to a file. Defaults to False.\n",
    "    current_index : int\n",
    "        Tracks the index of the current file being processed. Initialized to 0.\n",
    "    files : List[str]\n",
    "        List of PDF files in the directory. Initialized during object creation.\n",
    "    markdown : Optional[str]\n",
    "        Holds the markdown representation of the last processed file. Defaults to None.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __post_init__():\n",
    "        Initializes the list of PDF files in the directory. Raises FileNotFoundError if no PDF files are found.\n",
    "    \n",
    "    get_cv(path: str) -> str:\n",
    "        Converts a PDF file at the specified path to markdown format.\n",
    "    \n",
    "    get_next() -> Optional[str]:\n",
    "        Processes the next PDF file in the directory and converts it to markdown format.\n",
    "        If `save_to_file` is True, saves the markdown to a file. Returns the markdown or None if no files remain.\n",
    "    \n",
    "    reset():\n",
    "        Resets the processing index to the beginning and clears the last processed markdown.\n",
    "    \"\"\"\n",
    "\n",
    "    directory: str\n",
    "    save_to_file: bool = False\n",
    "    current_index: int = field(init=False, default=0)\n",
    "    files: List[str] = field(init=False)\n",
    "    markdown: Optional[str] = field(init=False, default=None)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the list of PDF files in the directory. Raises a FileNotFoundError\n",
    "        if no PDF files are found in the specified directory.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        FileNotFoundError\n",
    "            If no PDF files are found in the specified directory.\n",
    "        \"\"\"\n",
    "        self.files = [file for file in os.listdir(self.directory) if file.endswith('.pdf')]\n",
    "        if not self.files:\n",
    "            raise FileNotFoundError(\"No PDF files found in the specified directory.\")\n",
    "\n",
    "    def get_cv(self, path: str) -> str:\n",
    "        \"\"\"\n",
    "        Converts a PDF file at the specified path to markdown format.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path : str\n",
    "            Path to the PDF file to be converted.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            The markdown representation of the PDF file.\n",
    "        \"\"\"\n",
    "        self.markdown = pymupdf4llm.to_markdown(path, show_progress=False)\n",
    "        return self.markdown\n",
    "    \n",
    "    def get_next(self) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Processes the next PDF file in the directory and converts it to markdown format.\n",
    "        If `save_to_file` is True, saves the markdown to a file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Optional[str]\n",
    "            The markdown representation of the next PDF file, or None if no files remain.\n",
    "        \"\"\"\n",
    "        if self.current_index >= len(self.files):\n",
    "            return None\n",
    "        \n",
    "        current_file_path = os.path.join(self.directory, self.files[self.current_index])\n",
    "        self.current_index += 1\n",
    "        \n",
    "        self.markdown = self.get_cv(current_file_path)\n",
    "        \n",
    "        if self.save_to_file:\n",
    "            md_file_path = os.path.splitext(current_file_path)[0] + \".md\"\n",
    "            with open(md_file_path, 'w', encoding='utf-8') as md_file:\n",
    "                md_file.write(self.markdown)\n",
    "        \n",
    "        return self.markdown\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the processing index to the beginning and clears the last processed markdown.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.current_index = 0\n",
    "        self.markdown = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba13e24a-b2a8-46da-bc70-b5496dd3f636",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Handler():\n",
    "    resume: str\n",
    "\n",
    "    def split_by_blocks(self):\n",
    "        block_pattern = r'\\n\\n'\n",
    "        blocks = re.split(block_pattern, self.resume)\n",
    "        for idx, block in enumerate(blocks):\n",
    "            blocks[idx] = re.sub(r'#+\\s', '', block).lstrip('\\n')\n",
    "\n",
    "        blocks = list(filter(None, blocks))\n",
    "        return blocks\n",
    "    \n",
    "    def clean_resume_blocks(self):\n",
    "        \"\"\"\n",
    "        Cleans a list of text blocks by removing unwanted elements.\n",
    "        \n",
    "        Parameters:\n",
    "            blocks (list of str): The raw text blocks from a résumé.\n",
    "        \n",
    "        Returns:\n",
    "            list of str: The cleaned text blocks.\n",
    "        \"\"\"\n",
    "        extracted_links = []\n",
    "        unique_blocks = []\n",
    "        seen_blocks = set()\n",
    "        \n",
    "        for block in self.split_by_blocks():\n",
    "            # Remove special character sequences (e.g., '-----')\n",
    "            if re.match(r'^[\\-\\s]+$', block):\n",
    "                continue\n",
    "            \n",
    "            # Remove year ranges (e.g., '2022 - 2024', '2016 - Present')\n",
    "            if re.search(r'\\b\\d{4}\\s*-\\s*(\\d{4}|Present)\\b', block):\n",
    "                continue\n",
    "            \n",
    "            # Remove geographical data (e.g., 'Bs. As. Argentina')\n",
    "            if re.search(r'\\b(?:[A-Z][a-z]+\\.)+\\s*[A-Z][a-z]+(?:\\s*\\b[A-Z][a-z]+)?', block):\n",
    "                continue\n",
    "            \n",
    "            # Optionally, remove very short blocks (e.g., single words or short sequences)\n",
    "            if len(block.split()) < 3:\n",
    "                continue\n",
    "\n",
    "            # Find all links in the current block\n",
    "            links = re.findall(r'https?://[^\\s\\)\\]]+', block)\n",
    "            extracted_links.extend(links)\n",
    "    \n",
    "            # Remove links from the block\n",
    "            cleaned_block = re.sub(r'https?://[^\\s\\)\\]]+', '', block).strip()\n",
    "\n",
    "            # Remove leftover patterns like '[Some text] ()'\n",
    "            cleaned_block = re.sub(r'\\[([^\\[\\]]+)\\]\\s*\\(\\s*\\)', r'\\1', cleaned_block).strip()\n",
    "\n",
    "            # Normalize by removing newline characters and trimming extra spaces\n",
    "            normalized_block = ' '.join(cleaned_block.splitlines()).strip()\n",
    "    \n",
    "            # Check if the normalized version is already processed\n",
    "            if normalized_block not in seen_blocks:\n",
    "                seen_blocks.add(normalized_block)\n",
    "                unique_blocks.append(normalized_block)  # Keep the original formatting in the output\n",
    "            \n",
    "            text = \"\\n\".join(unique_blocks)\n",
    "            \n",
    "        return text, extracted_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bf6f373-8301-4311-a329-de18c7ed134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Profiler:\n",
    "    \"\"\"\n",
    "    A class for summarizing résumés into structured categories using AI models.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    resume : str\n",
    "        The raw text of the résumé to summarize.\n",
    "    save_to_file : bool, optional\n",
    "        If True, saves the summary to a file. Defaults to False.\n",
    "    model : str, optional\n",
    "        The AI model used for generating the summary. Defaults to 'llama3.2:1b'.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    summarize() -> str:\n",
    "        Summarizes the résumé into a structured format based on predefined categories.\n",
    "    \"\"\"\n",
    "\n",
    "    resume: str\n",
    "    save_to_file: bool = False\n",
    "    model: str = 'llama3.2:1b'\n",
    "    categories: List[str] = field(default_factory=lambda: ['contact', 'education', 'experience', 'skills'])\n",
    "        \n",
    "    def summarize(self) -> str:\n",
    "        \"\"\"\n",
    "        Summarizes the résumé into a structured format based on predefined categories.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            A structured summary of the résumé including name, skills, experience, and education.\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"You are a human resources expert, specialized in talent acquisition for schools.\n",
    "        You are tasked with summarizing résumés in the following structured format:\n",
    "        - Name: [Name here]\n",
    "        - Skills: [Skills listed here]\n",
    "        - Experience: [Job experience and other relevant experience here]\n",
    "        - Education: [Degrees obtained and courses taken]\n",
    "        \n",
    "        The summary should extract and organize the following details:\n",
    "        - Name: The candidate’s full name.\n",
    "        - Skills: List of technical and non-technical skills.\n",
    "        - Experience: Teaching/research positions, non-teaching roles, and any other relevant professional experience.\n",
    "        - Education: Degrees obtained and other studies, including courses taken.\n",
    "        \n",
    "        The following text is a candidate’s résumé:\n",
    "        \n",
    "        {self.resume}\n",
    "        \n",
    "        Provide the structured summary based on the given résumé. Do not output any explanatory text.\n",
    "        \"\"\"\n",
    "        response = ollama.generate(\n",
    "            model=self.model,\n",
    "            prompt=prompt\n",
    "        )\n",
    "        return response['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94a60d9f-3312-471d-ba2b-9e6c31104f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Juan Ignacio Beiroa\n",
      "- Name: Juan Ignacio Beiroa\n",
      "- Skills:\n",
      "    - Technical: Git, Excel, Python, Javascript, HTML, CSS\n",
      "    - Non-technical: Data Analytics & Machine Learning: Python (Pandas, Scikit-learn), SQL, Data Visualization (Dash, Plotly)\n",
      "- Experience:\n",
      "    - Teaching/Research positions:\n",
      "        - Data Professional, Physics Department, University of Buenos Aires\n",
      "        - Researcher and Advisor, Secretary General, Federal Council of Education\n",
      "    - Non-teaching roles:\n",
      "        - Professor and Coordinator, Secondary school\n",
      "    - Other relevant professional experience:\n",
      "        - Coordinator of the Science and Technology Department at Bayard School\n",
      "        - Data professional with a background in physics, education, and government advisory.\n",
      "- Education:\n",
      "    - Degrees obtained: University of Buenos Aires (Physics)\n",
      "    - Courses taken: Introduction to AI - Humai Institute, Spec. in Education Policies - Pedagogical University\n",
      "\n",
      "\n",
      "-------------------\n",
      "\n",
      "\n",
      "**Reúno información relevante sobre la vida y obra de cada uno de ellos**\n",
      "\n",
      "*   Beiroa J.I.\n",
      "    *   Laboratorio de Enseñanza de la Física\n",
      "    *   Discípulos: Ruiz N. & Zarza L.\n",
      "        *   Unidad Didáctica diseñada en la materia Didáctica General del Profesorado de Física de la FECyN. \n",
      "*   Dionofrio J.\n",
      "    *   Laboratorio de Enseñanza de la Física\n",
      "    *   Discípulos: Beiroa J.I.\n",
      "        *   Uso del GPS para el estudio del movimiento en Física.\n",
      "\n",
      "\n",
      "-------------------\n",
      "\n",
      "\n",
      "Name: Jibeiroa \n",
      "Skills: \n",
      "\n",
      "- Programming languages: Python, Java, C++\n",
      "- Non-technical skills: Data analysis, statistical software (R), programming languages (Python, Java)\n",
      "- Technical skills: MySQL, Arduino\n",
      "- Languages: Spanish, English\n",
      "- Other skills:\n",
      "  - Proficient in data visualization tools such as Tableau\n",
      "  - Experience with cloud computing platforms like AWS and Azure\n",
      "  - Familiarity with Agile development methodologies\n",
      "  - Knowledge of web scraping techniques\n",
      "  - Experience with machine learning algorithms\n",
      "\n",
      "Experience:\n",
      "\n",
      "- Teaching assistant at the Colegio Agustiniano Docente de Sistemas Tecnológicos, March 2017 - July 2018 \n",
      "- Assistant to the Department of Physics, Grupo de investigación, UBA (July 2018 - December 2019) \n",
      "- Ayudante de segunda, Departamento de Física, Facultad de Ciencias Exactas y Naturales, UBA (August 2017 - March 2018) \n",
      "- Technician at the Faculty of Sciences, UBA (March 2016 - August 2017)\n",
      "- Assistant to the Department of Physics, Grupo de investigación, UBA (January 2022 - June 2022)\n",
      "\n",
      "Education:\n",
      "\n",
      "- Bachelor's degree in Physics, Facultad de Ciencias Exactas y Naturales, UBA \n",
      "- Master's degree in Science and Technology Education, Universidad Pedagógica Nacional \n",
      "\n",
      "Other:\n",
      "\n",
      "- Coordinator of education at the IMPA Coordinador de educación (October 2020 - April 2022)\n",
      "- Teacher, Escuela Técnica ORT (2018 - December 2019) \n",
      "- Facilitator of teacher training courses, Instituto Superior de Educación Tecnológica (2016 - August 2017)\n",
      "- Research assistant at the University of Buenos Aires, BGH S.A. Control de calidad (March 2008 - March 2009)\n",
      "\n",
      "\n",
      "-------------------\n",
      "\n",
      "\n",
      "Juan Ignacio Beiroa\n",
      "- Skills: Python, Machine Learning, Data Analytics, SQL, Git, Excel, JavaScript, HTML, CSS, Arduino, Liderazgo Gestión de Proyectos\n",
      "- Experience:\n",
      "  + Científico de datos con experiencia en física y educación (2020 - Presente)\n",
      "  + Educador y ex asesor gubernamental en transición a la ciencia de datos. Busco aplicar habilidades analíticas, experiencia en aprendizajeautomático y una base sólida en Python para contribuir a soluciones basadas en datos.\n",
      "  - Instructor (2016 - Presente)\n",
      "  + Coordinador del Departamento de Ciencia y Tecnología\n",
      "  + Superviso a un equipo de cinco docentes \n",
      "  + Rediseño e implementación de los planes de estudio del área.\n",
      "\n",
      "\n",
      "-------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "getter = Getter(directory=cvs_path)\n",
    "cv_summaries = []\n",
    "\n",
    "while True:\n",
    "    cv = getter.get_next()\n",
    "    if cv is None:\n",
    "        break\n",
    "    handler = Handler(cv)\n",
    "    data = handler.clean_resume_blocks()\n",
    "    profiler = Profiler(data[0])\n",
    "    summary = profiler.summarize()\n",
    "    cv_summaries.append(summary)\n",
    "    print(summary)\n",
    "    print('\\n\\n-------------------\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03a82ced-0103-4eef-a4e6-146c616b50e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from mlx_lm import load, generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29b6db05-5462-4704-bcda-08ea03ed3f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3786b0d66c40778697553da07a1237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ff41813-7b13-4822-8529-646d502c14ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f52c7697d86b404a807d485da167dc5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modelFT, tokenizerFT = load('jbeiroa/mlx_lora_llama-3.2-1b-q8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e4996ec-351e-4ec1-9976-69c71771730d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlx_lm.models.llama.Model"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80af4e56-7e83-426d-831b-987e947ca371",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ProfilerFT:\n",
    "    \"\"\"\n",
    "    A class for summarizing résumés into structured categories using AI models.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    resume : str\n",
    "        The raw text of the résumé to summarize.\n",
    "    save_to_file : bool, optional\n",
    "        If True, saves the summary to a file. Defaults to False.\n",
    "    model : str, optional\n",
    "        The AI model used for generating the summary. Defaults to 'llama3.2:1b'.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    summarize() -> str:\n",
    "        Summarizes the résumé into a structured format based on predefined categories.\n",
    "    \"\"\"\n",
    "\n",
    "    resume: str\n",
    "    save_to_file: bool = False\n",
    "    model: str = 'llama3.2:1b'\n",
    "    tokenizer: str = 'tokenizer'\n",
    "    categories: List[str] = field(default_factory=lambda: ['contact', 'education', 'experience', 'skills'])\n",
    "        \n",
    "    def summarize(self) -> str:\n",
    "        \"\"\"\n",
    "        Summarizes the résumé into a structured format based on predefined categories.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            A structured summary of the résumé including name, skills, experience, and education.\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"You are a human resources expert, specialized in resume analysis and data extraction.\n",
    "        You are tasked with extracting information and summarizing résumés in the following structured format:\n",
    "        *Name*: [Name here]\n",
    "        *Skills*: [Skills listed here]\n",
    "        *Degree*: [Degrees obtained and relevant education information here]\n",
    "        *Companies worked at*: [Job experience and other relevant experience here]\n",
    "        *Designation*: [Job roles at companies worked at here]  \n",
    "        \n",
    "        Summarize the following resume using the specified structure:\n",
    "        {self.resume}\"\"\"\n",
    "        response = generate(\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            prompt=prompt\n",
    "        )\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca4c8f8f-714a-4398-a4d9-ac1470040006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of CuAlNi single crystals after ageing at 473K. Journal of Alloys and Compounds.\n",
      "Data Analytics & Machine Learning: Python (Pandas, Scikit-learn), SQL, Data Visualization (Dash, Plotly)\n",
      "Data professional with a background in physics, education, and government advisory. Proven expertise in data analytics, machine learning, Python, and SQL with hands-on experience developing impactful data solutions. Highly adaptable with strong project management and leadership skills, looking to leverage technical and analytical expertise in a consulting role focused on data-driven strategy and operational improvements.\n",
      "Technical Competencies: Git, Excel, Python, Javascript, HTML, CSS\n",
      "Data Analytics & Machine Learning: Python (Pandas, Scikit-learn), SQL, Data Visualization (Dash, Plotly)\n",
      "Data professional with a background in physics, education, and government advisory. Proven expertise in data analytics, machine learning, Python, and SQL with hands-on experience developing impactful data solutions. Highly adaptable with strong project management and leadership skills, looking to leverage technical and analytical expertise in a consulting role focused on data-driven strategy and operational improvements.\n",
      "Technical Competencies: Git, Excel, Python, Javascript, HTML, CSS\n",
      "Data Analytics & Machine Learning: Python (Pandas, Scikit-learn), SQL\n",
      "\n",
      "\n",
      "-------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-------------------\n",
      "\n",
      "\n",
      "\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "University of Buenos Aires\n",
      "\n",
      "\n",
      "\n",
      "-------------------\n",
      "\n",
      "\n",
      "Utilicé Python para compilar un conjunto de datos de 5000 estudiantes y construí un modelo de aprendizaje automático con una tasa de clasificación (MVP) aproximada del 75 %. Contribuí en negociaciones salariales con sindicatos mediante la realización de análisis salariales provinciales y cálculos de costos fiscales, lo que resultó en un importante aumento salarial real del 27% para los docentes en todo el país para mediados de 2023.\n",
      "Actualmente dentro de la cátedra de Física e Introducción a la Biofísica para las carreras de Medicina y Veterinaria.\n",
      "Física para Biólogos, Geólogos y Químicos del 2016 al 2017. Actualmente dentro de la cátedra de Física e Introducción a la Biofísica para las carreras de Medicina y Veterinaria.\n",
      "Física para Biólogos, Geólogos y Químicos del 2016 al 2017. Actualmente dentro de la cátedra de Física e Introducción a la Biofísica para las carreras de\n",
      "\n",
      "\n",
      "-------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "getter.reset()\n",
    "\n",
    "while True:\n",
    "    cv = getter.get_next()\n",
    "    if cv is None:\n",
    "        break\n",
    "    handler = Handler(cv)\n",
    "    data = handler.clean_resume_blocks()\n",
    "    profiler = ProfilerFT(data[0], model=modelFT, tokenizer=tokenizerFT)\n",
    "    summary = profiler.summarize()\n",
    "    cv_summaries.append(summary)\n",
    "    print(summary)\n",
    "    print('\\n\\n-------------------\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f2787f-017c-4518-9068-21997279423d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
